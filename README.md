# Awesome Visual Large Language Models (VLLMs)

## ðŸ”¥ðŸ”¥ðŸ”¥ Visual Large Language Models for Generalized and Specialized Applications

<p align="center">
    <img src="assets/VLM_revolution.png" width="90%" height="90%">
</p>

## ðŸ“¢ News

ðŸš€ **What's New in This Update**:

- [2024.12.30]: ðŸ”¥ We release our VLLM application paper list repo!

<span id="head-content"><font size=5><center><b> Table of Contents </b> </center></font></span>

- [Visual Large Language Models for Generalized and Specialized Applications](#awesome-vllms)
  - [Existing VLM surveys](#existing-vlm-surveys)
    - [VLM surveys](#vlm-surveys)
    - [MLLM surveys](#mllm-surveys)
  - [Vision-to-text](#vision-to-text)
    - [Image-to-text](#image-to-text)
      - [General domain](#general-domain)
        - [General ability](#general-ability)
        - [REC](#rec)
        - [RES](#res)
        - [OCR](#ocr)
        - [Retrieval](#retrieval)
      - [VLLM+X](#VLLM+X)
        - [Remote sensing](#remote-sensing)
        - [Medical](#medical)
        - [Science and math](#science-and-math)
        - [Graphics and UI](#graphics-and-ui)
        - [Financial analysis](#financial-analysis)
    - [Video-to-text](#video-to-text)
      - [General domain](#general-domain)
      - [Video conversation](#video-conversation)
      - [Egocentric understanding](#egocentric-understanding)
  - [Vision-to-action](#vision-to-action)
    - [Autonomous driving](#autonomous-driving)
      - [Perception](#perception)
      - [Planning](#planning)
      - [Prediction](#prediction)
    - [Embodied AI](#embodied-ai)
      - [Perception](#perception)
      - [Manipulation](#manipulation)
      - [Planning](#planning)
      - [Navigation](#navigation)
    - [Automated tool management](#automated-tool-management)
  - [Text-to-vision](#text-to-vision)
    - [Text-to-image](#text-to-image)
    - [Text-to-3D](#text-to-3D)
    - [Text-to-video](#text-to-video)
  - [Other applications](#other-applications)
    - [Face](#face)
    - [Anomaly Detetcion](#anomaly-detetcion)

## Existing VLM surveys

### VLM surveys

| Title                                                                                                                                                                                                                                                                                               |        Venue         |   Date    |  Code  |                              Project                               |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :-------: | :----: | :----------------------------------------------------------------: |
| ![Star](https://img.shields.io/github/stars/jingyi0000/VLM_survey.svg?style=social&label=Star) <br>[**Vision-Language Models for Vision Tasks: A Survey**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10445007) <br>                                                                  |        T-PAMI        | 2024-8-8  | Github |        [Project](https://github.com/jingyi0000/VLM_survey)         |
| ![Star](https://img.shields.io/github/stars/Yutong-Zhou-cv/Awesome-Text-to-Image.svg?style=social&label=Star) <br> [**Vision + Language Applications: A Survey**](https://openaccess.thecvf.com/content/CVPR2023W/GCV/papers/Zhou_Vision__Language_Applications_A_Survey_CVPRW_2023_paper.pdf) <br> |        CVPRW         | 2023-5-24 | Github | [Project](https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image) |
| [**Vision-and-Language Pretrained Models: A Survey**](https://arxiv.org/pdf/2204.07356.pdf) <br>                                                                                                                                                                                                    | IJCAI (survey track) | 2022-5-3  | Github |                              Project                               |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### MLLM surveys

| Title                                                                                                                                                                                                                                           |          Venue          |    Date    |  Code  |                                     Project                                     |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------: | :--------: | :----: | :-----------------------------------------------------------------------------: |
| ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star) <br> [**A Survey on Multimodal Large Language Models**](https://arxiv.org/pdf/2306.13549.pdf) <br>                    |         T-PAMI          | 2024-11-29 | Github | [Project](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)  |
| ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star) <br> [**MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs**](https://arxiv.org/pdf/2411.15296) <br> |          ArXiv          | 2024-11-22 | Github | [Project](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/) |
| [**A Survey on Multimodal Large Language Models**](https://academic.oup.com/nsr/advance-article-pdf/doi/10.1093/nsr/nwae403/60676453/nwae403.pdf) <br>                                                                                          | National Science Review | 2024-11-12 | Github |                                     Project                                     |
| [**Video Understanding with Large Language Models: A Survey**](https://arxiv.org/pdf/2312.17432) <br>                                                                                                                                           |          ArXiv          | 2024-6-24  | Github |  [Project](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)   |
| ![Star](https://img.shields.io/github/stars/HKUST-LongGroup/Awesome-MLLM-Benchmarks.svg?style=social&label=Star) <br> [**A Survey on Multimodal Benchmarks: In the Era of Large AI Models**](https://arxiv.org/pdf/2409.18142) <br>             |          ArXiv          | 2024-9-21  | Github |      [Project](https://github.com/HKUST-LongGroup/Awesome-MLLM-Benchmarks)      |
| ![Star](https://img.shields.io/github/stars/yunlong10/Awesome-LLMs-for-Video-Understanding.svg?style=social&label=Star) <br> [**The Revolution of Multimodal Large Language Models: A Survey**](https://arxiv.org/pdf/2402.12451) <br>          |          ArXiv          |  2024-6-6  | Github |                                     Project                                     |
| ![Star](https://img.shields.io/github/stars/lhanchao777/LVLM-Hallucinations-Survey.svg?style=social&label=Star) <br> [**A Survey on Hallucination in Large Vision-Language Models**](https://arxiv.org/pdf/2402.00253) <br>                     |          ArXiv          |  2024-5-6  | Github |      [Project](https://github.com/lhanchao777/LVLM-Hallucinations-Survey)       |
| [**Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions**](https://arxiv.org/pdf/2404.07214) <br>                                                                                          |          ArXiv          | 2024-4-12  | Github |                                     Project                                     |
| ![Star](https://img.shields.io/github/stars/MM-LLMs/mm-llms.github.io.svg?style=social&label=Star) <br> [**MM-LLMs: Recent Advances in MultiModal Large Language Models**](https://arxiv.org/pdf/2401.13601v4) <br>                             |          ArXiv          | 2024-2-20  | Github |                      [Project](https://mm-llms.github.io/)                      |
| [**Exploring the Reasoning Abilities of Multimodallarge Language Models (mllms): a Comprehensive survey on Emerging Trends in Multimodal Reasonings**](https://arxiv.org/pdf/2401.06805) <br>                                                   |          ArXiv          | 2024-1-18  | Github |                                     Project                                     |
| [**Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey**](https://arxiv.org/pdf/2312.16602) <br>                                                                                                                       |          ArXiv          | 2023-12-27 | Github |                                     Project                                     |
| [**Multimodal Large Language Models: A Survey**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10386743) <br>                                                                                                                        |         BigData         | 2023-12-15 | Github |                                     Project                                     |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

## Vision-to-text

### Image-to-text

#### General domain

##### General ability

| Model               | Title                                                                                                                                                    |  Venue  |    Date    |                                        Code                                        |                               Project                               |
| :------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :--------------------------------------------------------------------------------: | :-----------------------------------------------------------------: |
| CompCap             | [**CompCap: Improving Multimodal Large Language Models with Composite Captions**](https://arxiv.org/abs/2412.05243)                                      |  ArXiv  | 2024-12-06 |                                       Github                                       |                               Project                               |
| NVILA               | [**NVILA: Efficient Frontier Visual Language Models**](https://arxiv.org/abs/2412.04468)                                                                 |  ArXiv  | 2024-12-05 |                                       Github                                       |                               Project                               |
| Molmo and PixMo     | [**Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models**](https://arxiv.org/abs/2409.17146)                          |  ArXiv  | 2024-09-25 |                     [Github](https://github.com/allenai/molmo)                     |                               Project                               |
| Qwen2-VL            | [**Qwen2-VL: Enhancing Vision-Language Modelâ€™s Perception of the World at Any Resolution**](https://arxiv.org/abs/2408.03326)                            |  ArXiv  | 2024-09-18 |                    [Github](https://github.com/QwenLM/Qwen2-VL)                    |         [Project](https://qwenlm.github.io/blog/qwen2-vl/)          |
| mPLUG-Owl3          | [**mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models**](https://arxiv.org/pdf/2404.07214)                       |  ArXiv  | 2024-08-09 |                   [Github](https://github.com/X-PLUG/mPLUG-Owl)                    |                               Project                               |
| LLaVA-OneVision     | [**LLaVA-OneVision: Easy Visual Task Transfer**](https://arxiv.org/abs/2408.03326)                                                                       |  ArXiv  | 2024-08-06 | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/docs/LLaVA_OneVision.md) |                               Project                               |
| VILA$^{2}$          | [**VILA $^2$: VILA Augmented VILA**](https://arxiv.org/pdf/2407.17453)                                                                                   |  ArXiv  | 2024-07-24 |                                       Github                                       |                               Project                               |
| EVLM                | [**EVLM: An Efficient Vision-Language Model for Visual Understanding**](https://arxiv.org/pdf/2407.14177)                                                |  ArXiv  | 2024-07-19 |                                       Github                                       |                               Project                               |
| Cambrian-1          | [**Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs**](https://arxiv.org/pdf/2406.16860)                                          |  ArXiv  | 2024-06-24 |                [Github](https://github.com/cambrian-mllm/cambrian)                 |                               Project                               |
| Ovis                | [**Ovis: Structural Embedding Alignment for Multimodal Large Language Model**](https://arxiv.org/pdf/2405.20797)                                         |  ArXiv  | 2024-05-31 |                     [Github](https://github.com/AIDC-AI/Ovis)                      |                               Project                               |
| ConvLLaVA           | [**ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models**](https://arxiv.org/pdf/2405.15738)                                  |  ArXiv  | 2024-05-24 |                  [Github](https://github.com/alibaba/conv-llava)                   |                               Project                               |
| Meteor              | [**Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models**](https://arxiv.org/pdf/2405.15574v1)                                | NeurIPS | 2024-05-24 |                  [Github](https://github.com/ByungKwanLee/Meteor)                  |                               Project                               |
| CuMo                | [**CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts**](https://arxiv.org/pdf/2405.05949)                                                 |  ArXiv  | 2024-05-09 |                     [Github](https://github.com/SHI-Labs/CuMo)                     |                               Project                               |
| Mini-Gemini         | [**Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models**](https://arxiv.org/pdf/2403.18814)                                       |  ArXiv  | 2024-03-27 |                  [Github](https://github.com/dvlab-research/MGM)                   |              [Project](https://mini-gemini.github.io/)              |
| MM1                 | [**MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training**](https://arxiv.org/pdf/2403.09611)                                               |  ArXiv  | 2024-03-14 |                                       Github                                       |                               Project                               |
| DeepSeek-VL         | [**DeepSeek-VL: Towards Real-World Vision-Language Understanding**](https://arxiv.org/pdf/2403.05525)                                                    |  ArXiv  | 2024-03-08 |                [Github](https://github.com/deepseek-ai/DeepSeek-VL)                | [Project](https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B) |
| InternLM-XComposer2 | [**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**](https://arxiv.org/pdf/2401.16420) |  ArXiv  | 2024-01-29 |              [Github](https://github.com/InternLM/InternLM-XComposer)              |                               Project                               |
| MoE-LLaVA           | [**MoE-LLaVA: Mixture of Experts for Large Vision-Language Models**](https://arxiv.org/pdf/2401.15947)                                                   |  ArXiv  | 2024-01-29 |                [Github](https://github.com/PKU-YuanGroup/MoE-LLaVA)                |                               Project                               |
| InternVL            | [**InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks**](https://arxiv.org/pdf/2312.14238)                   |  CVPR   | 2023-12-21 |                  [Github](https://github.com/OpenGVLab/InternVL)                   |        [Project](https://internvl.readthedocs.io/en/latest/)        |
| VILA                | [**VILA: On Pre-training for Visual Language Models**](https://arxiv.org/pdf/2312.07533)                                                                 |  ArXiv  | 2023-12-12 |                      [Github](https://github.com/NVlabs/VILA)                      |                               Project                               |
| Vary                | [**Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models**](https://arxiv.org/pdf/2312.06109)                                          |  ECCV   | 2023-12-11 |                  [Github](https://github.com/Ucas-HaoranWei/Vary)                  |                               Project                               |
| Honeybee            | [**Honeybee: Locality-enhanced Projector for Multimodal LLM**](https://arxiv.org/pdf/2312.06742)                                                         |  CVPR   | 2023-11-11 |                    [Github](https://github.com/khanrc/honeybee)                    |                               Project                               |
| OtterHD             | [**OtterHD: A High-Resolution Multi-modality Model**](https://arxiv.org/pdf/2311.04219)                                                                  |  ArXiv  | 2023-11-07 |                     [Github](https://github.com/Luodian/Otter)                     |                               Project                               |
| mPLUG-Owl2          | [**mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration**](https://arxiv.org/pdf/2404.07214)                         |  CVPR   | 2023-11-07 |                   [Github](https://github.com/X-PLUG/mPLUG-Owl)                    |                               Project                               |
| Fuyu                | [**Fuyu-8B: A Multimodal Architecture for AI Agents**](https://www.adept.ai/blog/fuyu-8b)                                                                |  ArXiv  | 2023-10-17 |                   [Github](https://huggingface.co/adept/fuyu-8b)                   |            [Project](https://www.adept.ai/blog/fuyu-8b)             |
| MiniGPT-v2          | [**MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning**](https://arxiv.org/pdf/2310.09478)                  |  ArXiv  | 2023-10-14 |                 [Github](https://github.com/Vision-CAIR/MiniGPT-4)                 |               [Project](https://minigpt-4.github.io/)               |
| LLaVA 1.5           | [**Improved Baselines with Visual Instruction Tuning**](https://arxiv.org/pdf/2310.03744)                                                                |  ArXiv  | 2023-10-05 |                   [Github](https://github.com/haotian-liu/LLaVA)                   |               [Project](https://llava-vl.github.io/)                |
| InternLM-XComposer  | [**InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition**](https://arxiv.org/pdf/2309.15112)          |  ArXiv  | 2023-09-26 |              [Github](https://github.com/InternLM/InternLM-XComposer)              |                               Project                               |
| Qwen-VL             | [**Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond**](https://arxiv.org/pdf/2308.12966)             |  ArXiv  | 2023-08-24 |                    [Github](https://github.com/QwenLM/Qwen-VL)                     |                               Project                               |
| StableLLaVA         | [**StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data**](https://arxiv.org/pdf/2308.10253)                             |  ArXiv  | 2023-08-20 |                  [Github](https://github.com/icoz69/StableLLAVA)                   |      [Project](https://icoz69.github.io/stablellava-official/)      |
| BLIVA               | [**BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions**](https://arxiv.org/pdf/2308.09936)                                 |  AAAI   | 2023-08-19 |                    [Github](https://github.com/mlpc-ucsd/BLIVA)                    |                               Project                               |
| SVIT                | [**SVIT: Scaling up Visual Instruction Tuning**](https://arxiv.org/pdf/2307.04087)                                                                       |  ArXiv  | 2023-07-09 |          [Github](https://github.com/BAAI-DCAI/Visual-Instruction-Tuning)          |                               Project                               |
| LaVIN               | [**Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models**](https://arxiv.org/pdf/2305.15023)                          | NeurIPS | 2023-05-24 |                   [Github](https://github.com/luogen1996/LaVIN)                    |                               Project                               |
| InstructBLIP        | [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://arxiv.org/pdf/2404.07214)                             | NeurIPS | 2023-05-11 |   [Github](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)    |                               Project                               |
| MultiModal-GPT      | [**MultiModal-GPT: A Vision and Language Model for Dialogue with Humans**](https://arxiv.org/pdf/2404.07214)                                             |  ArXiv  | 2023-05-08 |               [Github](https://github.com/open-mmlab/Multimodal-GPT)               |                               Project                               |
| Otter               | [**Otter: A Multi-Modal Model with In-Context Instruction Tuning**](https://arxiv.org/pdf/2305.03726)                                                    |  ArXiv  | 2023-05-05 |                     [Github](https://github.com/Luodian/Otter)                     |                               Project                               |
| mPLUG-Owl           | [**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**](https://arxiv.org/pdf/2304.14178)                                      |  ArXiv  | 2023-04-27 |                   [Github](https://github.com/X-PLUG/mPLUG-Owl)                    |                               Project                               |
| LLaMA-Adapter V2    | [**LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model**](https://arxiv.org/pdf/2304.15010)                                                   |  ArXiv  | 2023-04-28 |              [Github](https://github.com/ZrrSkywalker/LLaMA-Adapter)               |                               Project                               |
| MiniGPT-4           | [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/pdf/2404.07214)                           | NeurIPS | 2023-04-20 |                 [Github](https://github.com/Vision-CAIR/MiniGPT-4)                 |               [Project](https://minigpt-4.github.io/)               |
| LLaVA               | [**Visual Instruction Tuning**](https://arxiv.org/pdf/2304.08485)                                                                                        | NeurIPS | 2023-04-17 |                   [Github](https://github.com/haotian-liu/LLaVA)                   |               [Project](https://llava-vl.github.io/)                |
| LLaMA-Adapter       | [**LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention**](https://arxiv.org/pdf/2303.16199)                    |  ICLR   | 2023-03-28 |              [Github](https://github.com/ZrrSkywalker/LLaMA-Adapter)               |                               Project                               |
| Kosmos-1            | [**Language Is Not All You Need: Aligning Perception with Language Models**](https://arxiv.org/pdf/2302.14045)                                           | NeurIPS | 2023-02-27 |                    [Github](https://github.com/microsoft/unilm)                    |                               Project                               |
| Flamingo            | [**Flamingo: a Visual Language Model for Few-Shot Learning**](https://arxiv.org/pdf/2204.14198)                                                          | NeurIPS | 2022-04-29 |              [Github](https://github.com/mlfoundations/open_flamingo)              |                               Project                               |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### REC

| Model       | Title                                                                                                                                                |  Venue  |    Date    |                                          Code                                           |                         Project                         |
| :---------- | :--------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :-------------------------------------------------------------------------------------: | :-----------------------------------------------------: |
| OMG-LLaVA   | [**OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding**](https://arxiv.org/pdf/2406.19389)                       | NeurIPS | 2024-06-27 |                       [Github](https://github.com/lxtGH/OMG-Seg)                        |  [Project](https://lxtgh.github.io/project/omg_llava/)  |
| VisionLLMv2 | [**VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks**](https://arxiv.org/pdf/2406.08394) |  ArXiv  | 2024-06-12 |                    [Github](https://github.com/OpenGVLab/VisionLLM)                     | [Project](https://shramanpramanick.github.io/VistaLLM/) |
| LLM-Seg     | [**LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning**](https://arxiv.org/pdf/2404.08767)                                      |  CVPR   | 2024-04-12 |      [Github](https://www.google.com/search?q=LLM-Seg%5C&sourceid=chrome&ie=UTF-8)      |                         Project                         |
| PSALM       | [**PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model**](https://arxiv.org/pdf/2403.14598)                                                   |  ECCV   | 2024-03-21 |                       [Github](https://github.com/zamling/PSALM)                        |          [Project](https://pixellm.github.io/)          |
| GROUNDHOG   | [**GROUNDHOG: Grounding Large Language Models to Holistic Segmentation**](https://arxiv.org/pdf/2402.16846)                                          |  CVPR   | 2024-02-26 |                      [Github](https://github.com/LeapLabTHU/GSVA)                       |      [Project](https://groundhog-mllm.github.io/)       |
| GELLA       | [**Generalizable Entity Grounding via Assistance of Large Language Model**](https://arxiv.org/pdf/2402.02555)                                        |  ECCV   | 2024-02-04 |                                         Github                                          |                         Project                         |
| OMG-Seg     | [**OMG-Seg: Is One Model Good Enough For All Segmentation?**](https://arxiv.org/pdf/2401.10229)                                                      |  CVPR   | 2024-01-18 |                       [Github](https://github.com/lxtGH/OMG-Seg)                        |   [Project](https://lxtgh.github.io/project/omg_seg/)   |
| LISA++      | [**LISA++: An Improved Baseline for Reasoning Segmentation with Large Language Model**](https://arxiv.org/pdf/2312.17240)                            |  ArXiv  | 2023-12-28 |                    [Github](https://github.com/dvlab-research/LISA)                     |                         Project                         |
| VistaLLM    | [**Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model**](https://arxiv.org/pdf/2312.12423)            |  ECCV   | 2023-12-19 |                    [Github](https://github.com/OpenGVLab/VisionLLM)                     | [Project](https://shramanpramanick.github.io/VistaLLM/) |
| Osprey      | [**Osprey: Pixel Understanding with Visual Instruction Tuning**](https://arxiv.org/pdf/2312.10032)                                                   |  CVPR   | 2023-12-15 |                     [Github](https://github.com/CircleRadon/Osprey)                     |                         Project                         |
| GSVA        | [**GSVA: Generalized Segmentation via Multimodal Large Language Models**](https://arxiv.org/abs/2312.10103)                                          |  CVPR   | 2023-12-05 |                      [Github](https://github.com/LeapLabTHU/GSVA)                       |                         Project                         |
| PixelLM     | [**PixelLM: Pixel Reasoning with Large Multimodal Model**](https://arxiv.org/pdf/2312.02228)                                                         |  CVPR   | 2023-12-04 |                    [Github](https://github.com/MaverickRen/PixelLM)                     |          [Project](https://pixellm.github.io/)          |
| PixelLLM    | [**PixelLM: Pixel Reasoning with Large Multimodal Model**](https://arxiv.org/pdf/2312.02228)                                                         |  ECCV   | 2023-12-04 | [Github](https://github.com/google-research/scenic/tree/main/scenic/projects/pixel_llm) |        [Project](https://jerryxu.net/PixelLLM/)         |
| LLaFS       | [**LLaFS: When Large Language Models Meet Few-Shot Segmentation**](https://arxiv.org/pdf/2311.16926)                                                 |  CVPR   | 2023-11-28 |                     [Github](https://github.com/lanyunzhu99/LLaFS)                      |                         Project                         |
| NExT-Chat   | [**NExT-Chat: An LMM for Chat, Detection and Segmentation**](https://arxiv.org/pdf/2311.04498)                                                       |  ArXiv  | 2023-11-08 |                    [Github](https://github.com/NExT-ChatV/NExT-Chat)                    |        [Project](https://next-chatv.github.io/)         |
| GLaMM       | [**GLaMM: Pixel Grounding Large Multimodal Model**](https://arxiv.org/pdf/2311.03356)                                                                |  CVPR   | 2023-11-06 |                  [Github](https://github.com/mbzuai-oryx/groundingLMM)                  | [Project](https://mbzuai-oryx.github.io/groundingLMM/)  |
| LISA        | [**LISA: Reasoning Segmentation via Large Language Model**](https://arxiv.org/pdf/2308.00692)                                                        |  CVPR   | 2023-08-01 |                    [Github](https://github.com/dvlab-research/LISA)                     |                         Project                         |
| ContextDET  | [**Contextual Object Detection with Multimodal Large Language Models**](https://arxiv.org/pdf/2305.18279)                                            |  ArXiv  | 2023-05-29 |                   [Github](https://github.com/yuhangzang/ContextDET)                    |                         Project                         |
| VisionLLM   | [**VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks**](https://arxiv.org/pdf/2305.11175)                       |  ArXiv  | 2023-05-18 |                    [Github](https://github.com/OpenGVLab/VisionLLM)                     |                         Project                         |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### RES

| Model               | Title                                                                                                                                                |  Venue  |    Date    | Code                                                              | Project                                                 |
| :------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :---------------------------------------------------------------- | :------------------------------------------------------ |
| **ChatRex**         | [**ChatRex: Taming Multimodal LLM for Joint Perception and Understanding**](https://arxiv.org/abs/2411.18363)                                        |  ArXiv  | 2024-11-27 | [Github](https://github.com/IDEA-Research/ChatRex)                | Project                                                 |
| **Griffon-G**       | [**Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models**](https://arxiv.org/abs/2410.16163)                     |  ArXiv  | 2024-10-21 | [Github](https://github.com/jefferyZhan/Griffon)                  | Project                                                 |
| **Ferret**          | [**Ferret: Refer and Ground Anything Anywhere at Any Granularity**](https://arxiv.org/pdf/2310.07704)                                                |  ICLR   | 2024-10-11 | [Github](https://github.com/apple/ml-ferret)                      | Project                                                 |
| **OMG-LLaVA**       | [**OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding**](https://arxiv.org/pdf/2406.19389)                       | NeurIPS | 2024-06-27 | [Github](https://github.com/lxtGH/OMG-Seg)                        | [Project](https://lxtgh.github.io/project/omg_llava/)   |
| **VisionLLMv2**     | [**VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks**](https://arxiv.org/pdf/2406.08394) |  ArXiv  | 2024-06-12 | [Github](https://github.com/OpenGVLab/VisionLLM)                  | Project                                                 |
| **Groma**           | [**Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models**](https://arxiv.org/abs/2404.13013)                          |  ECCV   | 2024-04-19 | [Github](https://github.com/FoundationVision/Groma)               | Project                                                 |
| **Griffonv2**       | [**Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring**](https://arxiv.org/abs/2403.09333)    |  ArXiv  | 2024-03-14 | [Github](https://github.com/jefferyZhan/Griffon)                  | Project                                                 |
| **ASMv2**           | [**The All-Seeing Project V2: Towards General Relation Comprehension of the Open World**](https://arxiv.org/pdf/2402.19474)                          |  ECCV   | 2024-02-29 | [Github](https://github.com/OpenGVLab/all-seeing)                 | Project                                                 |
| **SPHINX-X**        | [**SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models**](https://arxiv.org/abs/2402.05935)                      |  ArXiv  | 2024-02-08 | [Github](https://github.com/sphinx-doc/sphinx)                    | Project                                                 |
| **ChatterBox**      | [**ChatterBox: Multi-round Multimodal Referring and Grounding**](https://arxiv.org/abs/2401.13307)                                                   |  ArXiv  | 2024-01-24 | [Github](https://github.com/sunsmarterjie/ChatterBox)             | Project                                                 |
| **LEGO**            | [**LEGO: Language Enhanced Multi-modal Grounding Model**](https://arxiv.org/pdf/2401.06071v2)                                                        |  ArXiv  | 2024-01-12 | [Github](https://github.com/standardgalactic/lego)                | Project                                                 |
| **GroundingGPT**    | [**GroundingGPT: Language Enhanced Multi-modal Grounding Model**](https://arxiv.org/abs/2401.06071)                                                  |   ACL   | 2024-01-11 | [Github](https://github.com/standardgalactic/lego)                | Project                                                 |
| **BuboGPT**         | [**BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs**](https://arxiv.org/pdf/2307.08581)                                                       |  ArXiv  | 2024-07-17 | [Github](https://github.com/magic-research/bubogpt)               | [Project](https://bubo-gpt.github.io/)                  |
| **Ferret-v2**       | [**Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models**](https://arxiv.org/pdf/2404.07973)                       |  COLM   | 2024-04-11 | [Github](https://github.com/apple/ml-ferret)                      | Project                                                 |
| **InfMLLM**         | [**InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory**](https://arxiv.org/pdf/2402.04617)                   | NeurIPS | 2024-02-07 | [Github](https://github.com/thunlp/InfLLM)                        | Project                                                 |
| **VistaLLM**        | [**Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model**](https://arxiv.org/pdf/2312.12423)            |  ECCV   | 2023-12-19 | Github                                                            | [Project](https://shramanpramanick.github.io/VistaLLM/) |
| **LLaVA-Grounding** | [**LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models**](https://arxiv.org/abs/2312.02949)                                           |  ArXiv  | 2023-12-05 | [Github](https://github.com/UX-Decoder/LLaVA-Grounding)           | [Project](https://llava-vl.github.io/llava-grounding/)  |
| **Lenna**           | [**Lenna: Language Enhanced Reasoning Detection Assistant**](https://arxiv.org/abs/2312.02433)                                                       |  ArXiv  | 2023-12-05 | [Github](https://github.com/Meituan-AutoML/Lenna)                 | Project                                                 |
| **Griffon**         | [**Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models**](https://arxiv.org/abs/2311.14552)                     |  ECCV   | 2023-11-24 | [Github](https://github.com/jefferyZhan/Griffon)                  | Project                                                 |
| **Lion**            | [**Lion: Empowering multimodal large language model with dual-level visual knowledge**](https://arxiv.org/abs/2311.11860)                            |  CVPR   | 2023-11-20 | [Github](https://github.com/JiuTian-VL/JiuTian-LION)              | Project                                                 |
| **SPHINX**          | [**SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models**](https://arxiv.org/abs/2311.07575)      |  ArXiv  | 2023-11-13 | [Github]()                                                        | Project                                                 |
| **NExT-Chat**       | [**NExT-Chat: An LMM for Chat, Detection and Segmentation**](https://arxiv.org/pdf/2311.04498)                                                       |  ArXiv  | 2023-11-08 | [Github](https://github.com/NExT-ChatV/NExT-Chat)                 | [Project](https://next-chatv.github.io/)                |
| **GLaMM**           | [**GLaMM: Pixel Grounding Large Multimodal Model**](https://arxiv.org/pdf/2311.03356)                                                                |  CVPR   | 2023-11-06 | [Github](https://github.com/mbzuai-oryx/groundingLMM)             | [Project](https://mbzuai-oryx.github.io/groundingLMM/)  |
| **CogVLM**          | [**CogVLM: Visual Expert for Pretrained Language Models**](https://arxiv.org/pdf/2311.03079)                                                         |  ArXiv  | 2023-11-06 | [Github](https://github.com/THUDM/CogVLM)                         | Project                                                 |
| **Pink**            | [**Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs**](https://arxiv.org/pdf/2310.00582)                                  |  CVPR   | 2023-10-01 | [Github](https://github.com/SY-Xuan/Pink)                         | Project                                                 |
| **PVIT**            | [**Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models**](https://arxiv.org/pdf/2308.13437)                             |  ArXiv  | 2023-08-25 | [Github](https://github.com/PVIT-official/PVIT)                   | Project                                                 |
| **ASM**             | [**The all-seeing project: Towards panoptic visual recognition and understanding of the open world**](https://arxiv.org/pdf/2308.01907)              |  ICLR   | 2023-08-03 | [Github](https://github.com/OpenGVLab/all-seeing)                 | Project                                                 |
| **Shikra**          | [**Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic**](https://arxiv.org/pdf/2306.15195)                                               |  ArXiv  | 2023-06-27 | [Github](https://github.com/shikras/shikra)                       | Project                                                 |
| **Kosmos-2**        | [**KOSMOS-2: Grounding Multimodal Large LanguageModels to the World**](https://arxiv.org/pdf/2306.14824)                                             |  ICLR   | 2023-06-26 | [Github](https://github.com/microsoft/unilm/tree/master/kosmos-2) | Project                                                 |
| **ChatSpot**        | [**ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning**](https://arxiv.org/pdf/2307.09474)                             |  ArXiv  | 2023-07-18 | [Github](https://github.com/Ahnsun/ChatSpot)                      | Project                                                 |
| **GPT4RoI**         | [**GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest**](https://arxiv.org/pdf/2307.03601)                                       |  IJCAI  | 2023-07-07 | [Github](https://github.com/jshilong/GPT4RoI)                     | Project                                                 |
| **ContextDET**      | [**Contextual Object Detection with Multimodal Large Language Models**](https://arxiv.org/pdf/2305.18279)                                            |  ArXiv  | 2023-05-29 | [Github](https://github.com/yuhangzang/ContextDET)                | Project                                                 |
| **DetGPT**          | [**DetGPT: Detect What You Need via Reasoning**](https://arxiv.org/pdf/2305.14167)                                                                   |  ArXiv  | 2023-05-23 | [Github](https://github.com/OptimalScale/DetGPT)                  | [Project](https://detgpt.github.io/)                    |
| **VisionLLM**       | [**VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks**](https://arxiv.org/pdf/2305.11175)                       |  ArXiv  | 2023-05-18 | [Github](https://github.com/OpenGVLab/VisionLLM)                  | Project                                                 |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### OCR

| Model                    | Title                                                                                                                                                         |  Venue  |    Date    |                                             Code                                             |               Project                |
| :----------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------ | :-----: | :--------: | :------------------------------------------------------------------------------------------: | :----------------------------------: |
| TextHawk2                | [**TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens**](https://arxiv.org/abs/2410.05261)                  |  ArXiv  | 2024-10-07 |                         [Github](https://github.com/yuyq96/TextHawk)                         |               Project                |
| Dockylin                 | [**DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming**](https://arxiv.org/abs/2406.19101)                   |  AAAI   | 2024-06-27 |                       [Github](https://github.com/ZZZHANG-jx/DocKylin)                       |               Project                |
| StrucTexTv3              | [**StrucTexTv3: An Efficient Vision-Language Model for Text-rich Image Perception, Comprehension, and Beyond**](https://arxiv.org/abs/2405.21013)             |  ArXiv  | 2024-05-31 |                                          [Github]()                                          |               Project                |
| Fox                      | [**Focus Anywhere for Fine-grained Multi-page Document Understanding**](https://arxiv.org/abs/2405.14295)                                                     |  ArXiv  | 2024-05-23 |                           [Github](https://github.com/ucaslcl/Fox)                           |               Project                |
| TextMonkey               | [**TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document**](https://arxiv.org/abs/2403.04473)                                             |  ArXiv  | 2024-05-07 | [Github](https://github.com/Yuliang-Liu/Monkey/blob/main/monkey_model/text_monkey/README.md) |               Project                |
| TinyChart                | [**TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning**](https://arxiv.org/abs/2404.16635)                   |   ACL   | 2024-04-25 |                       [Github](https://github.com/X-PLUG/mPLUG-DocOwl)                       |               Project                |
| TextHawk                 | [**TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models**](https://arxiv.org/abs/2404.09204)                             |  ArXiv  | 2024-04-14 |                         [Github](https://github.com/yuyq96/TextHawk)                         |               Project                |
| HRVDA                    | [**HRVDA: High-Resolution Visual Document Assistant**](https://arxiv.org/abs/2404.06918)                                                                      |  CVPR   | 2024-04-10 |                                          [Github]()                                          |               Project                |
| InternLM-XComposer2-4KHD | [**InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD**](https://arxiv.org/abs/2404.06512)      | NeurIPS | 2024-04-09 |                   [Github](https://github.com/InternLM/InternLM-XComposer)                   |               Project                |
| LayoutLLM                | [**LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding**](https://arxiv.org/abs/2404.05225)                            |  CVPR   | 2024-04-08 |                                          [Github]()                                          |               Project                |
| ViTLP                    | [**Visually Guided Generative Text-Layout Pre-training for Document Intelligence**](https://arxiv.org/abs/2403.16516)                                         |  NAACL  | 2024-03-25 |                    [Github](https://github.com/Veason-silverbullet/ViTLP)                    |               Project                |
| mPLUG-DocOwl 1.5         | [**mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding**](https://arxiv.org/abs/2403.12895)                                      |  ArXiv  | 2024-03-19 |                       [Github](https://github.com/X-PLUG/mPLUG-DocOwl)                       |               Project                |
| DoCo                     | [**Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models**](https://arxiv.org/abs/2402.19014)                     |  CVPR   | 2024-02-29 |                                          [Github]()                                          |               Project                |
| TGDoc                    | [**Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs**](https://arxiv.org/abs/2311.13194)                                  |  ArXiv  | 2023-11-22 |                         [Github](https://github.com/harrytea/TGDoc)                          |               Project                |
| DocPedia                 | [**DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding**](https://arxiv.org/abs/2311.11810) |  ArXiv  | 2023-11-20 |                                          [Github]()                                          |               Project                |
| UReader                  | [**UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model**](https://arxiv.org/abs/2310.05126)             |   ACL   | 2023-10-08 |                    [Github](https://github.com/LukeForeverYoung/UReader)                     |               Project                |
| UniDoc                   | [**UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding**](https://arxiv.org/abs/2308.11592)   |  ArXiv  | 2023-08-19 |                                          [Github]()                                          |               Project                |
| mPLUG-DocOwl             | [**mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding**](https://arxiv.org/abs/2307.02499)                                  |  ArXiv  | 2023-07-04 |                       [Github](https://github.com/X-PLUG/mPLUG-DocOwl)                       |               Project                |
| LLaVAR                   | [**LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding**](https://arxiv.org/abs/2306.17107)                                          |  ArXiv  | 2023-06-29 |                         [Github](https://github.com/SALT-NLP/LLaVAR)                         | [Project](https://llavar.github.io/) |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### Retrieval

| Model      | Title                                                                                                               | Venue |    Date    |                      Code                       |                     Project                      |
| :--------- | :------------------------------------------------------------------------------------------------------------------ | :---: | :--------: | :---------------------------------------------: | :----------------------------------------------: |
| EchoSight  | [**EchoSight: Advancing Visual-Language Models with Wiki Knowledge**](https://arxiv.org/abs/2407.12735)             | EMNLP | 2024-07-17 | [Github](https://github.com/Go2Heart/EchoSight) | [Project](https://go2heart.github.io/echosight/) |
| FROMAGe    | [**Grounding Language Models to Images for Multimodal Inputs and Outputs**](https://arxiv.org/abs/2301.13823)       | ICML  | 2024-01-31 | [Github](https://github.com/kohjingyu/fromage)  |                     Project                      |
| Wiki-LLaVA | [**Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs**](https://arxiv.org/abs/2404.15406) | CVPR  | 2023-04-23 |                   [Github]()                    |                     Project                      |
| UniMuR     | [**Unified Embeddings for Multimodal Retrieval via Frozen LLMs**](https://arxiv.org/abs/1905.03197)                 | ICML  | 2019-05-08 |                   [Github]()                    |                     Project                      |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### VLLM + X

##### Remote sensing

| Name      | Title                                                                                                                                                                                                                                                                 |    Venue     |    Date    |                         Code                         |                      Project                      |
| :-------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------: | :--------: | :--------------------------------------------------: | :-----------------------------------------------: |
| VHM       | ![Star](https://img.shields.io/github/stars/opendatalab/VHM.svg?style=social&label=Star) <br> [**VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis**](https://arxiv.org/pdf/2403.20213) <br>                                          |    ArXiv     | 2024-11-06 |     [Github](https://github.com/opendatalab/VHM)     | [Project](https://fitzpchao.github.io/vhm_page/)  |
| LHRS-Bot  | ![Star](https://img.shields.io/github/stars/NJU-LHRS/LHRS-Bot.svg?style=social&label=Star) <br> [**LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model**](https://arxiv.org/pdf/2402.02544) <br>                                    |     ECCV     | 2024-07-16 |    [Github](https://github.com/NJU-LHRS/LHRS-Bot)    |                      Project                      |
| Popeye    | [**Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery**](https://arxiv.org/pdf/2403.03790) <br>                                                                                                                      |   J-STARS    | 2024-06-13 |                        Github                        |                      Project                      |
| RS-LLaVA  | ![Star](https://img.shields.io/github/stars/BigData-KSU/RS-LLaVA.svg?style=social&label=Star) <br> [**RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery**](https://www.mdpi.com/2072-4292/16/9/1477) <br> | Remote Sens. | 2024-04-23 |  [Github](https://github.com/BigData-KSU/RS-LLaVA)   |                      Project                      |
| EarthGPT  | ![Star](https://img.shields.io/github/stars/wivizhang/EarthGPT.svg?style=social&label=Star) <br> [**EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain**](https://arxiv.org/pdf/2401.16822) <br>    |     TGRS     | 2024-03-08 |   [Github](https://github.com/wivizhang/EarthGPT)    |                      Project                      |
| RS-CapRet | [**Large Language Models for Captioning and Retrieving Remote Sensing Images**](https://arxiv.org/pdf/2402.06475) <br>                                                                                                                                                |    ArXiv     | 2024-02-09 |                        Github                        |                      Project                      |
| SkyEyeGPT | ![Star](https://img.shields.io/github/stars/ZhanYang-nwpu/SkyEyeGPT.svg?style=social&label=Star) <br> [**SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model**](https://arxiv.org/pdf/2401.09712) <br>          |    ArXiv     | 2024-01-18 | [Github](https://github.com/ZhanYang-nwpu/SkyEyeGPT) |                      Project                      |
| GeoChat   | ![Star](https://img.shields.io/github/stars/mbzuai-oryx/geochat.svg?style=social&label=Star) <br> [**GeoChat: Grounded Large Vision-Language Model for Remote Sensing**](https://arxiv.org/pdf/2311.15826) <br>                                                       |     CVPR     | 2023-11-24 |   [Github](https://github.com/mbzuai-oryx/geochat)   | [Project](https://mbzuai-oryx.github.io/GeoChat/) |
| RSGPT     | ![Star](https://img.shields.io/github/stars/Lavender105/RSGPT.svg?style=social&label=Star) <br> [**RSGPT: A Remote Sensing Vision Language Model and Benchmark**](https://arxiv.org/pdf/2307.15266) <br>                                                              |    ArXiv     | 2023-07-28 |    [Github](https://github.com/Lavender105/RSGPT)    |                      Project                      |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### Medical

| Name           | Title                                                                                                                                                                                                                                               |  Venue  |    Date    |                          Code                           | Project |
| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :-----------------------------------------------------: | :-----: |
| PMC-VQA        | ![Star](https://img.shields.io/github/stars/xiaoman-zhang/PMC-VQA.svg?style=social&label=Star) <br> [**PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering**](https://arxiv.org/pdf/2305.10415) <br>                           |  ArXiv  | 2024-09-08 |   [Github](https://github.com/xiaoman-zhang/PMC-VQA)    | Project |
| MedVersa       | [**A Generalist Learner for Multifaceted Medical Image Interpretation**](https://arxiv.org/pdf/2405.07988) <br>                                                                                                                                     |  ArXiv  | 2024-05-13 |                         Github                          | Project |
| PeFoMed        | ![Star](https://img.shields.io/github/stars/jinlHe/PeFoMed.svg?style=social&label=Star) <br> [**PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging**](https://arxiv.org/pdf/2401.02797) <br>          |  ArXiv  | 2024-04-16 |       [Github](https://github.com/jinlHe/PeFoMed)       | Project |
| RaDialog       | ![Star](https://img.shields.io/github/stars/ChantalMP/RaDialog.svg?style=social&label=Star) <br> [**RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance**](https://arxiv.org/pdf/2311.18681) <br> |  ArXiv  | 2023-11-30 |     [Github](https://github.com/ChantalMP/RaDialog)     | Project |
| Med-Flamingo   | ![Star](https://img.shields.io/github/stars/snap-stanford/med-flamingo.svg?style=social&label=Star) <br> [**Med-Flamingo: a Multimodal Medical Few-shot Learner**](https://arxiv.org/pdf/2307.15189) <br>                                           |  ML4H   | 2023-07-27 | [Github](https://github.com/snap-stanford/med-flamingo) | Project |
| XrayGPT        | ![Star](https://img.shields.io/github/stars/mbzuai-oryx/XrayGPT.svg?style=social&label=Star) <br> [**XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models**](https://arxiv.org/pdf/2306.07971) <br>                        | BioNLP  | 2023-06-13 |    [Github](https://github.com/mbzuai-oryx/XrayGPT)     | Project |
| LLaVA-Med      | ![Star](https://img.shields.io/github/stars/microsoft/LLaVA-Med.svg?style=social&label=Star) <br> [**LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day**](https://arxiv.org/pdf/2306.00890) <br>                 | NeurIPS | 2023-06-01 |    [Github](https://github.com/microsoft/LLaVA-Med)     | Project |
| CXR-RePaiR-Gen | [**Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models**](https://arxiv.org/pdf/2305.03660) <br>                                                                                                                              |  MLHC   | 2023-05-05 |                         Github                          | Project |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### Science and math

| Name           | Title                                                                                                                                                                                                                                                                 |  Venue  |    Date    |                                Code                                 |                   Project                    |
| :------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :-----------------------------------------------------------------: | :------------------------------------------: |
| MAVIS          | ![Star](https://img.shields.io/github/stars/ZrrSkywalker/MAVIS.svg?style=social&label=Star) <br> [**MAVIS: Mathematical Visual Instruction Tuning**](https://arxiv.org/pdf/2407.08739) <br>                                                                           |  ECCV   | 2024-11-01 |           [Github](https://github.com/ZrrSkywalker/MAVIS)           |                   Project                    |
| Math-LLaVA     | ![Star](https://img.shields.io/github/stars/HZQ950419/Math-LLaVA.svg?style=social&label=Star) <br> [**Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models**](https://arxiv.org/pdf/2406.172940) <br>                                |  EMNLP  | 2024-10-08 |          [Github](https://github.com/HZQ950419/Math-LLaVA)          |                   Project                    |
| MathVerse      | ![Star](https://img.shields.io/github/stars/ZrrSkywalker/MathVerse.svg?style=social&label=Star) <br> [**MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?**](https://arxiv.org/pdf/2403.14624) <br>                                |  ECCV   | 2024-08-18 |         [Github](https://github.com/ZrrSkywalker/MathVerse)         | [Project](https://mathverse-cuhk.github.io/) |
| We-Math        | ![Star](https://img.shields.io/github/stars/We-Math/We-Math.svg?style=social&label=Star) <br> [**We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?**](https://arxiv.org/pdf/2407.01284) <br>                                       |  ArXiv  | 2024-07-01 |            [Github](https://github.com/We-Math/We-Math)             |    [Project](https://we-math.github.io/)     |
| CMMaTH         | [**CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models**](https://arxiv.org/pdf/2407.12023) <br>                                                                                                                                      |  ArXiv  | 2024-06-28 |                               Github                                |                   Project                    |
| GeoEval        | ![Star](https://img.shields.io/github/stars/GeoEval/GeoEval.svg?style=social&label=Star) <br> [**GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving**](https://arxiv.org/pdf/2402.10104) <br>                                  |   ACL   | 2024-05-17 |            [Github](https://github.com/GeoEval/GeoEval)             |                   Project                    |
| FigurA11y      | ![Star](https://img.shields.io/github/stars/allenai/figura11y.svg?style=social&label=Star) <br> [**FigurA11y: AI Assistance for Writing Scientific Alt Text**](https://dl.acm.org/doi/10.1145/3640543.3645212) <br>                                                   |   IUI   | 2024-04-05 |           [Github](https://github.com/allenai/figura11y)            |                   Project                    |
| MathVista      | ![Star](https://img.shields.io/github/stars/lupantech/MathVista.svg?style=social&label=Star) <br> [**MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts**](https://arxiv.org/pdf/2310.02255) <br>                                   |  ICLR   | 2024-01-21 |          [Github](https://github.com/lupantech/MathVista)           |   [Project](https://mathvista.github.io/)    |
| mPLUG-PaperOwl | ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl.svg?style=social&label=Star) <br> [**mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model**](https://arxiv.org/pdf/2311.18248) <br>                                   | ACM MM  | 2024-01-09 | [Github](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl) |                   Project                    |
| G-LLaVA        | ![Star](https://img.shields.io/github/stars/pipilurj/G-LLaVA.svg?style=social&label=Star) <br> [**G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model**](https://arxiv.org/pdf/2312.11370) <br>                                                  |  ArXiv  | 2023-12-18 |            [Github](https://github.com/pipilurj/G-LLaVA)            |                   Project                    |
| T-SciQ         | ![Star](https://img.shields.io/github/stars/T-SciQ/T-SciQ.svg?style=social&label=Star) <br> [**T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering**](https://arxiv.org/pdf/2305.03453) <br> |  AAAI   | 2023-12-18 |             [Github](https://github.com/T-SciQ/T-SciQ)              |                   Project                    |
| ScienceQA      | ![Star](https://img.shields.io/github/stars/lupantech/ScienceQA.svg?style=social&label=Star) <br> [**Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering**](https://arxiv.org/pdf/2209.09513) <br>                               | NeurIPS | 2022-10-17 |          [Github](https://github.com/lupantech/ScienceQA)           |   [Project](https://scienceqa.github.io/)    |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### Graphics and UI

| Name      | Title                                                                                                                                                                                            | Venue |    Date    |                          Code                           | Project |
| :-------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--------: | :-----------------------------------------------------: | :-----: |
| Graphist  | ![Star](https://img.shields.io/github/stars/graphic-design-ai/graphist.svg?style=social&label=Star) <br> [**Graphic Design with Large Multimodal Model**](https://arxiv.org/pdf/2404.14368) <br> | ArXiv | 2024-04-22 | [Github](https://github.com/graphic-design-ai/graphist) | Project |
| Ferret-UI | [**Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs**](https://arxiv.org/pdf/2404.05719) <br>                                                                                    | ECCV  | 2024-04-08 |                         Github                          | Project |
| CogAgent  | ![Star](https://img.shields.io/github/stars/THUDM/CogVLM.svg?style=social&label=Star) <br> [**CogAgent: A Visual Language Model for GUI Agents**](https://arxiv.org/pdf/2312.08914) <br>         | CVPR  | 2023-12-21 |        [Github](https://github.com/THUDM/CogVLM)        | Project |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

##### Financial analysis

| Name       | Title                                                                                                                                                                                                                   | Venue |    Date    |                      Code                      | Project |
| :--------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--------: | :--------------------------------------------: | :-----: |
| FinTral    | [**FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models**](https://arxiv.org/pdf/2402.10986) <br>                                                                                                |  ACL  | 2024-06-14 |                     Github                     | Project |
| FinVis-GPT | ![Star](https://img.shields.io/github/stars/wwwadx/FinVis-GPT.svg?style=social&label=Star) <br> [**FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis**](https://arxiv.org/pdf/2308.01430) <br> | ArXiv | 2023-07-31 | [Github](https://github.com/wwwadx/FinVis-GPT) | Project |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### Video-to-text

#### General domain

| Name           | Title                                                                                                                                                                                                                     |     Venue     |    Date    |                                         Code                                          |                          Project                           |
| :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :-----------: | :--------: | :-----------------------------------------------------------------------------------: | :--------------------------------------------------------: |
| Video-LLaMA2   | ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2) <br> [VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/pdf/2406.07476)                     |     ArXiv     | 2024-10-30 |                 [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA2)                 |                          Project                           |
| LongVU         | ![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU) <br> [LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/pdf/2410.17434)                                |     ArXiv     | 2024-10-22 |                    [Github](https://github.com/Vision-CAIR/LongVU)                    |      [Project](https://vision-cair.github.io/LongVU)       |
| Goldfish       | ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT4-video) <br> [Goldfish: Vision-Language Understanding of Arbitrarily Long Videos](https://arxiv.org/abs/2407.12679)                                       |     ECCV      | 2024-07-17 |               [Github](https://vision-cair.github.io/Goldfish_website/)               | [Project](https://vision-cair.github.io/Goldfish_website/) |
| MiniGPT4-Video | ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT4-video) <br> [MiniGPT4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens](https://arxiv.org/pdf/2404.03413) | CVPR Workshop | 2024-04-04 |                [Github](https://github.com/Vision-CAIR/MiniGPT4-video)                |  [Project](https://vision-cair.github.io/MiniGPT4-video/)  |
| ST-LLM         | ![Star](https://img.shields.io/github/stars/TencentARC/ST-LLM) <br> [ST-LLM: Large language models are effective temporal learners](https://arxiv.org/pdf/2404.00308)                                                     |     ECCV      | 2024-03-30 |                    [Github](https://github.com/TencentARC/ST-LLM)                     |                          Project                           |
| LLaMA-VID      | ![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID) <br> [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)                                             |     ECCV      | 2023-11-28 |                 [Github](https://github.com/dvlab-research/LLaMA-VID)                 |          [Project](https://llama-vid.github.io/)           |
| Video-LLaMA    | ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA) <br> [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/pdf/2306.02858)                       |     EMNLP     | 2023-10-25 |                 [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA)                  |                          Project                           |
| Vid2Seq        | ![Star](https://img.shields.io/github/stars/google-research/scenic) <br> [Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning](https://arxiv.org/pdf/2302.14115)                       |     CVPR      | 2023-03-21 | [Github](https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq) |     [Project](https://antoyang.github.io/vid2seq.html)     |
| LaViLa         | ![Star](https://img.shields.io/github/stars/facebookresearch/LaViLa) <br> [Learning Video Representations from Large Language Models](https://arxiv.org/abs/2212.04501)                                                   |     CVPR      | 2022-12-08 |                 [Github](https://github.com/facebookresearch/LaViLa)                  |   [Project](https://facebookresearch.github.io/LaViLa/)    |
| VideoBERT      | ![Star](https://img.shields.io/github/stars/ammesatyajit/VideoBERT) <br> [VideoBERT: A joint model for video and language representation learning](https://arxiv.org/pdf/1904.01766) <br>                                 |     ICCV      | 2019-09-11 |                  [Github](https://github.com/ammesatyajit/VideoBERT)                  |                          Project                           |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Video conversation

| Name | Title | Venue | Date | Code | Project |
| :--- | :---- | :---: | :--: | :--: | :-----: |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Egocentric view

| Name | Title | Venue | Date | Code | Project |
| :--- | :---- | :---: | :--: | :--: | :-----: |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

## Vision-to-action

### Autonomous driving

#### Perception

| Name        | Title                                                                                                                                                                                                                                                                          | Venue |    Date    |                         Code                         |                      Project                       |
| :---------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--------: | :--------------------------------------------------: | :------------------------------------------------: |
| DriveLM     | ![Star](https://img.shields.io/github/stars/OpenDriveLab/DriveLM.svg?style=social&label=Star) <br> [**DriveLM: Driving with Graph Visual Question Answering**](https://arxiv.org/pdf/2312.14150) <br>                                                                          | ECCV  | 2024-7-17  |  [Github](https://github.com/OpenDriveLab/DriveLM)   |                      Project                       |
| Talk2BEV    | ![Star](https://img.shields.io/github/stars/llmbev/talk2bev.svg?style=social&label=Star) <br> [**Talk2BEV: Language-enhanced Birdâ€™s-eye View Maps for Autonomous Driving**](https://arxiv.org/pdf/2310.02251) <br>                                                             | ICRA  | 2024-5-13  |     [Github](https://github.com/llmbev/talk2bev)     |   [Project](https://llmbev.github.io/talk2bev/)    |
| Nuscenes-QA | ![Star](https://img.shields.io/github/stars/qiantianwen/NuScenes-QA.svg?style=social&label=Star) <br> [**TNuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario**](https://ojs.aaai.org/index.php/AAAI/article/view/28253/28499) <br> | AAAI  | 2024-3-24  | [Github](https://github.com/qiantianwen/NuScenes-QA) |                      Project                       |
| DriveMLM    | ![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM.svg?style=social&label=Star) <br> [**DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving**](https://arxiv.org/pdf/2312.09245) <br>                      | ArXiv | 2023-12-25 |   [Github](https://github.com/OpenGVLab/DriveMLM)    |                      Project                       |
| LiDAR-LLM   | [**LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding**](https://arxiv.org/pdf/2312.14074v1) <br>                                                                                                                                          | CoRR  | 2023-12-21 |                        Github                        | [Project](https://sites.google.com/view/lidar-llm) |
| Dolphis     | ![Star](https://img.shields.io/github/stars/SaFoLab-WISC/Dolphins.svg?style=social&label=Star) <br> [**Dolphins: Multimodal Language Model for Driving**](https://arxiv.org/abs/2312.00438) <br>                                                                               | ArXiv | 2023-12-1  |  [Github](https://github.com/SaFoLab-WISC/Dolphins)  |      [Project](https://vlm-driver.github.io/)      |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Planning

| Name          | Title                                                                                                                                                                                                                                                                                 |      Venue       |    Date    |                                              Code                                              |                                   Project                                    |
| :------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------: | :--------: | :--------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------: |
| DriveGPT4     | [**DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model**](https://arxiv.org/pdf/2311.13549) <br>                                                                                                                                                          |       RAL        |  2024-8-7  | [Github](https://drive.google.com/drive/folders/1PsGL7ZxMMz1ZPDS5dZSjzjfPjuPHxVL5?usp=sharing) | [Project](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10629039) |
| SurrealDriver | ![Star](https://img.shields.io/github/stars/AIR-DISCOVER/Driving-Thinking-Dataset.svg?style=social&label=Star) <br> [**SurrealDriver: Designing LLM-powered Generative Driver Agent Framework based on Human Driversâ€™ Driving-thinking Data**](https://arxiv.org/pdf/2309.13193) <br> |      ArXiv       | 2024-7-22  |               [Github](https://github.com/AIR-DISCOVER/Driving-Thinking-Dataset)               |                                   Project                                    |
| DriveVLM      | [**DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models**](https://arxiv.org/abs/2402.12289) <br>                                                                                                                                                         |       CoRL       | 2024-6-25  |                                             Github                                             |           [Project](https://tsinghua-mars-lab.github.io/DriveVLM/)           |
| DiLu          | ![Star](https://img.shields.io/github/stars/PJLab-ADG/DiLu.svg?style=social&label=Star) <br> [**DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models**](https://arxiv.org/pdf/2309.16292) <br>                                                          |       ICLR       | 2024-2-22  |                          [Github](https://github.com/PJLab-ADG/DiLu)                           |                 [Project](https://pjlab-adg.github.io/DiLu/)                 |
| LMDrive       | ![Star](https://img.shields.io/github/stars/opendilab/LMDrive.svg?style=social&label=Star) <br> [**LMDrive: Closed-Loop End-to-End Driving with Large Language Models**](https://arxiv.org/pdf/2309.13193) <br>                                                                       |       CVPR       | 2023-12-21 |                         [Github](https://github.com/opendilab/LMDrive)                         |            [Project](https://hao-shao.com/projects/lmdrive.html)             |
| GPT-Driver    | ![Star](https://img.shields.io/github/stars/PointsCoder/GPT-Driver.svg?style=social&label=Star) <br> [**DGPT-Driver: Learning to Drive with GPT**](https://arxiv.org/abs/2402.12289) <br>                                                                                             | NeurlPS Workshop | 2023-12-5  |                      [Github](https://github.com/PointsCoder/GPT-Driver)                       |   [Project](https://pointscoder.github.io/projects/gpt_driver/index.html)    |
| ADriver-I     | [**ADriver-I: A General World Model for Autonomous Driving**](https://arxiv.org/pdf/2311.13549) <br>                                                                                                                                                                                  |      ArXiv       | 2023-11-22 |                                             Github                                             |                                   Project                                    |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Prediction

| Name           | Title                                                                                                                                                                                                                                                                                                                                                                                    | Venue |   Date   |                           Code                           | Project |
| :------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :------: | :------------------------------------------------------: | :-----: |
| BEV-InMLLM     | ![Star](https://img.shields.io/github/stars/xmed-lab/NuInstruct.svg?style=social&label=Star) <br> [**Holistic autonomous driving understanding by birdâ€™s-eye-view injected multi-Modal large model**](https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_Holistic_Autonomous_Driving_Understanding_by_Birds-Eye-View_Injected_Multi-Modal_Large_Models_CVPR_2024_paper.pdf) <br> | CVPR  | 2024-1-2 |     [Github](https://github.com/xmed-lab/NuInstruct)     | Project |
| Prompt4Driving | ![Star](https://img.shields.io/github/stars/wudongming97/Prompt4Driving.svg?style=social&label=Star) <br> [**Language Prompt for Autonomous Driving**](https://arxiv.org/pdf/2309.04379) <br>                                                                                                                                                                                            | ArXiv | 2023-9-8 | [Github](https://github.com/wudongming97/Prompt4Driving) | Project |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### Embodied AI

#### Perception

| Name           | Title                                                                                                                                                                                                                                                                                                                               | Venue |   Date    |                                     Code                                     |                         Project                          |
| :------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :-------: | :--------------------------------------------------------------------------: | :------------------------------------------------------: |
| Wonderful-Team | ![Star](https://img.shields.io/github/stars/wonderful-team-robotics/wonderful_team_robotics.svg?style=social&label=Star) <br> [**Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs**](https://arxiv.org/pdf/2407.19094) <br>                                                                                        | ArXiv | 2024-12-4 | [Github](https://github.com/wonderful-team-robotics/wonderful_team_robotics) |   [Project](https://wonderful-team-robotics.github.io)   |
| AffordanceLLM  | ![Star](https://img.shields.io/github/stars/wj-on-un/AffordanceLLM_implementation.svg?style=social&label=Star) <br> [**AffordanceLLM: Grounding Affordance from Vision Language Models**](https://arxiv.org/abs/2401.06341) <br>                                                                                                    | CVPR  | 2024-4-17 |      [Github](https://github.com/wj-on-un/AffordanceLLM_implementation)      |   [Project](https://jasonqsy.github.io/AffordanceLLM/)   |
| 3DVisProg      | ![Star](https://img.shields.io/github/stars/CurryYuan/ZSVG3D.svg?style=social&label=Star) <br> [**Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding**](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) <br> | CVPR  | 2024-3-23 |                [Github](https://github.com/CurryYuan/ZSVG3D)                 |      [Project](https://curryyuan.github.io/ZSVG3D/)      |
| WREPLAN        | [**REPLAN: Robotic Replanning with Perception and Language Models**](https://arxiv.org/pdf/2401.04157) <br>                                                                                                                                                                                                                         | ArXiv | 2024-2-20 |                                    Github                                    | [Project](https://replan-lm.github.io/replan.github.io/) |
| PaLM-E         | [**PaLM-E: An Embodied Multimodal Language Model**](https://arxiv.org/pdf/2303.03378) <br>                                                                                                                                                                                                                                          | ICML  | 2023-3-6  |                                    Github                                    |           [Project](https://palm-e.github.io)            |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Manipulation

| Name         | Title                                                                                                                                                                                                                                            | Venue |    Date    |                              Code                              |                       Project                       |
| :----------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--------: | :------------------------------------------------------------: | :-------------------------------------------------: |
| OpenVLA      | ![Star](https://img.shields.io/github/stars/openvla/openvla.svg?style=social&label=Star) <br> [**OpenVLA: An Open-Source Vision-Language-Action Model**](https://arxiv.org/pdf/2406.09246) <br>                                                  | ArXiv |  2024-9-5  |          [Github](https://github.com/openvla/openvla)          |        [Project](https://openvla.github.io)         |
| LLARVA       | ![Star](https://img.shields.io/github/stars/Dantong88/LLARVA.svg?style=social&label=Star) <br> [**LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning**](https://arxiv.org/pdf/2406.11815) <br>                                     | CoRL  | 2024-6-17  |         [Github](https://github.com/Dantong88/LLARVA)          |        [Project](https://llarva24.github.io)        |
| RT-X         | ![Star](https://img.shields.io/github/stars/google-deepmind/open_x_embodiment.svg?style=social&label=Star) <br> [**Open X-Embodiment: Robotic Learning Datasets and RT-X Models**](https://arxiv.org/pdf/2310.08864) <br>                        | ArXiv |  2024-6-1  | [Github](https://github.com/google-deepmind/open_x_embodiment) | [Project](https://robotics-transformer2.github.io)  |
| RoboFlamingo | [**Vision-Language Foundation Models as Effective Robot Imitators**](https://arxiv.org/pdf/2311.01378) <br>                                                                                                                                      | ICLR  |  2024-2-5  |                             Github                             |                       Project                       |
| VoxPoser     | ![Star](https://img.shields.io/github/stars/huangwl18/VoxPoser.svg?style=social&label=Star) <br> [**VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models**](https://arxiv.org/pdf/2307.05973) <br>                   | CoRL  | 2023-11-2  |        [Github](https://github.com/huangwl18/VoxPoser)         |        [Project](https://voxposer.github.io)        |
| ManipLLM     | ![Star](https://img.shields.io/github/stars/clorislili/ManipLLM.svg?style=social&label=Star) <br> [**ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation**](https://arxiv.org/pdf/2312.16217) <br>        | CVPR  | 2023-12-24 |        [Github](https://github.com/clorislili/ManipLLM)        |  [Project](https://sites.google.com/view/manipllm)  |
| RT-2         | [**RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**](https://robotics-transformer2.github.io/assets/rt2.pdf) <br>                                                                                                 | ArXiv | 2023-7-28  |                             Github                             | [Project](https://robotics-transformer-x.github.io) |
| Instruct2Act | ![Star](https://img.shields.io/github/stars/OpenGVLab/Instruct2Act.svg?style=social&label=Star) <br> [**Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model**](https://arxiv.org/pdf/2305.11176) <br> | ArXiv | 2023-5-24  |      [Github](https://github.com/OpenGVLab/Instruct2Act)       |                       Project                       |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Planning

| Name        | Title                                                                                                                                                                                                                                                                                                                                                 |  Venue  |    Date    |                             Code                             |                       Project                       |
| :---------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :----------------------------------------------------------: | :-------------------------------------------------: |
| LLaRP       | ![Star](https://img.shields.io/github/stars/apple/ml-llarp.svg?style=social&label=Star) <br> [**Large Language Models as Generalizable Policies for Embodied Tasks**](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) <br>                       |  ICLR   | 2024-4-16  |         [Github](https://github.com/apple/ml-llarp)          |         [Project](https://llm-rl.github.io)         |
| MP5         | ![Star](https://img.shields.io/github/stars/IranQin/MP5.svg?style=social&label=Star) <br> [**MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception**](https://arxiv.org/pdf/2312.07472) <br>                                                                                                                               |  CVPR   | 2024-3-24  |           [Github](https://github.com/IranQin/MP5)           | [Project](https://iranqin.github.io/MP5.github.io/) |
| LL3DA       | ![Star](https://img.shields.io/github/stars/Open3DA/LL3DA.svg?style=social&label=Star) <br> [**LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning**](https://arxiv.org/pdf/2311.18651) <br>                                                                                                              |  CVPR   | 2023-11-30 |          [Github](https://github.com/Open3DA/LL3DA)          |         [Project](https://ll3da.github.io/)         |
| EmbodiedGPT | ![Star](https://img.shields.io/github/stars/EmbodiedGPT/EmbodiedGPT_Pytorch.svg?style=social&label=Star) <br> [**EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought**](https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf) <br> | NeurlPS | 2023-11-2  | [Github](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch) |      [Project](https://embodiedgpt.github.io)       |
| ELLM        | ![Star](https://img.shields.io/github/stars/yuqingd/ellm.svg?style=social&label=Star) <br> [**Guiding Pretraining in Reinforcement Learning with Large Language Models**](https://proceedings.mlr.press/v202/du23f/du23f.pdf) <br>                                                                                                                    |  ICML   | 2023-9-15  |          [Github](https://github.com/yuqingd/ellm)           |                       Project                       |
| 3D-LLM      | ![Star](https://img.shields.io/github/stars/UMass-Foundation-Model/3D-LLM.svg?style=social&label=Star) <br> [**3D-LLM: Injecting the 3D World into Large Language Models**](https://arxiv.org/abs/2307.12981) <br>                                                                                                                                    | NeurlPS | 2023-7-24  |  [Github](https://github.com/UMass-Foundation-Model/3D-LLM)  |   [Project](hhttps://vis-www.cs.umass.edu/3dllm/)   |
| NLMap       | ![Star](https://img.shields.io/github/stars/ericrosenbrown/nlmap_spot.svg?style=social&label=Star) <br> [**Open-vocabulary Queryable Scene Representations for Real World Planning**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161534) <br>                                                                                         |  ICRA   |  2023-7-4  |    [Github](https://github.com/ericrosenbrown/nlmap_spot)    |      [Project](https://nlmap-saycan.github.io)      |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

#### Navigation

| Name          | Title                                                                                                                                                                                                                                                                        | Venue |   Date    |                             Code                             |                      Project                      |
| :------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :-------: | :----------------------------------------------------------: | :-----------------------------------------------: |
| ConceptGraphs | ![Star](https://img.shields.io/github/stars/concept-graphs/concept-graphs.svg?style=social&label=Star) <br> [**ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10610243) <br>         | ICRA  | 2024-5-13 |  [Github](https://github.com/concept-graphs/concept-graphs)  |    [Project](https://concept-graphs.github.io)    |
| RILA          | [**RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation**](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf) <br> | CVPR  | 2024-4-27 |                            Github                            |                      Project                      |
| EMMA          | ![Star](https://img.shields.io/github/stars/stevenyangyj/Emma-Alfworld.svg?style=social&label=Star) <br> [**Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld**](https://arxiv.org/abs/2401.08577) <br>                                                 | CVPR  | 2024-3-29 |   [Github](https://github.com/stevenyangyj/Emma-Alfworld)    |                      Project                      |
| VLN-VER       | ![Star](https://img.shields.io/github/stars/DefaultRui/VLN-VER.svg?style=social&label=Star) <br> [**Volumetric Environment Representation for Vision-Language Navigation**](https://arxiv.org/pdf/2403.14158) <br>                                                           | CVPR  | 2024-3-24 |       [Github](https://github.com/DefaultRui/VLN-VER)        |                      Project                      |
| MultiPLY      | ![Star](https://img.shields.io/github/stars/UMass-Foundation-Model/MultiPLY.svg?style=social&label=Star) <br> [**MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World**](https://arxiv.org/abs/2401.08577) <br>                                 | CVPR  | 2024-1-16 | [Github](https://github.com/UMass-Foundation-Model/MultiPLY) | [Project](https://vis-www.cs.umass.edu/multiply/) |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### Automated tool management

| Name          | Title                                                                                                                                                                                                                           |            Venue             |    Date    |                         Code                         |                    Project                     |
| :------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------: | :--------: | :--------------------------------------------------: | :--------------------------------------------: |
| TROVE         | ![Star](https://img.shields.io/github/stars/zorazrw/trove.svg?style=social&label=Star) <br> [**TROVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks**](https://arxiv.org/abs/2401.12869)           |            arXiv             | 2024-01-23 |      [Github](https://github.com/zorazrw/trove)      |                    Project                     |
| Tool-LMM      | ![Star](https://img.shields.io/github/stars/MLLM-Tool/MLLM-Tool.svg?style=social&label=Star) <br> [**Tool-LMM: A Large Multi-Modal Model for Tool Agent Learning**](https://arxiv.org/abs/2401.10727)                           |            arXiv             | 2024-1-19  |   [Github](https://github.com/MLLM-Tool/MLLM-Tool)   |                    Project                     |
| CLOVA         | ![Star](https://img.shields.io/github/stars/clova-tool/CLOVA-tool.svg?style=social&label=Star) <br> [**CLOVA: A Closed-loop Visual Assistant with Tool Usage and Update**](https://arxiv.org/abs/2312.10908)                    |             CVPR             | 2023-12-18 |  [Github](https://github.com/clova-tool/CLOVA-tool)  |    [Project](https://clova-tool.github.io/)    |
| CRAFT         | ![Star](https://img.shields.io/github/stars/lifan-yuan/CRAFT.svg?style=social&label=Star) <br> [**CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets**](https://arxiv.org/abs/2309.17428)             |            arXiv             | 2023-9-29  |    [Github](https://github.com/lifan-yuan/CRAFT)     |                    Project                     |
| Confucius     | ![Star](https://img.shields.io/github/stars/shizhl/CTL.svg?style=social&label=Star) <br> [**Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum**](https://arxiv.org/abs/2308.14034) |             AAAI             | 2023-8-27  |       [Github](https://github.com/shizhl/CTL)        |                    Project                     |
| AVIS          | [**Avis: Autonomous visual information seeking with large language model agent**](https://arxiv.org/abs/2306.08129)                                                                                                             |           NeurIPS            | 2023-6-13  |                        Github                        |                    Project                     |
| GPT4Tools     | ![Star](https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?style=social&label=Star) <br> [**GPT4Tools: Teaching large language model to use tools via self-instruction**](https://arxiv.org/abs/2305.18752)          |           NeurIPS            | 2023-5-30  |  [Github](https://github.com/StevenGrove/GPT4Tools)  |                    Project                     |
| ToolkenGPT    | ![Star](https://img.shields.io/github/stars/Ber666/ToolkenGPT.svg?style=social&label=Star) <br> [**ToolkenGPT: Augmenting frozen language models with massive tools via tool embeddings**](https://arxiv.org/abs/2305.11554)    |           NeurIPS            | 2023-5-19  |    [Github](https://github.com/Ber666/ToolkenGPT)    |                    Project                     |
| Chameleon     | ![Star](https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&label=Star) <br> [**Chameleon: Plug-and-play compositional reasoning with large language models**](https://arxiv.org/abs/2304.09842)       |           NeurIPS            | 2023-4-19  | [Github](https://github.com/lupantech/chameleon-llm) |  [Project](https://chameleon-llm.github.io/)   |
| HuggingGPT    | ![Star](https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&label=Star) <br> [**HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face**](https://arxiv.org/abs/2303.17580)                |           NeurIPS            | 2023-3-30  |    [Github](https://github.com/microsoft/JARVIS)     |                    Project                     |
| TaskMatrix.AI | [**TaskMatrix.AI: Completing tasks by connecting foundation models with millions of APIs**](https://arxiv.org/abs/2303.16434)                                                                                                   | Intelligent Computing (AAAS) | 2023-3-29  |                        Github                        |                    Projecct                    |
| MM-ReACT      | ![Star](https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&label=Star) <br> [**MM-ReACT: Prompting ChatGPT for Multimodal Reasoning and Action**](https://arxiv.org/abs/2303.11381)                        |            arXiv             | 2023-3-20  |   [Github](https://github.com/microsoft/MM-REACT)    | [Project](https://multimodal-react.github.io/) |
| ViperGPT      | ![Star](https://img.shields.io/github/stars/cvlab-columbia/viper.svg?style=social&label=Star) <br> [**ViperGPT: Visual Inference via Python Execution for Reasoning**](https://arxiv.org/abs/2303.08128)                        |             ICCV             | 2023-3-14  |  [Github](https://github.com/cvlab-columbia/viper)   |                    Project                     |
| MINDâ€™S EYE    | [**MINDâ€™S EYE: GROUNDED LANGUAGE MODEL REASONING THROUGH SIMULATION**](https://arxiv.org/abs/2210.05359)                                                                                                                        |            arXiv             | 2022-10-11 |                        GitHub                        |                    Project                     |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

## Text-to-vision

### Text-to-image

| Name         | Title                                                                                                                                                                                                                                                                        |  Venue  |    Date    |                              Code                               |                            Project                            |
| :----------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :--------: | :-------------------------------------------------------------: | :-----------------------------------------------------------: |
| LLMGA        | ![Star](https://img.shields.io/github/stars/dvlab-research/LLMGA.svg?style=social&label=Star) <br> [**LLMGA: Multimodal Large Language Model based Generation Assistant**](https://arxiv.org/pdf/2311.16500)                                                                 |  ECCV   | 2024-7-27  |        [GitHub](https://github.com/dvlab-research/LLMGA)        |              [Project](https://llmga.github.io/)              |
| Emu          | ![Star](https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&label=Star) <br> [**Generative pretraining in multimodality,**](https://arxiv.org/pdf/2307.05222)                                                                                                |  ICLR   |  2024-5-8  |           [GitHub](https://github.com/baaivision/Emu)           |                            Project                            |
| Kosmos-G     | [**Kosmos-G: Generating Images in Context with Multimodal Large Language Models**](https://arxiv.org/pdf/2310.02992)                                                                                                                                                         |  ICLR   | 2024-4-26  |                             GitHub                              |              [Project](https://aka.ms/GeneralAI)              |
| LaVIT        | ![Star](https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&label=Star) <br> [**Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**](https://arxiv.org/pdf/2309.04669)                                                      |  ICLR   | 2024-3-22  |            [GitHub](https://github.com/jy0205/LaVIT)            |                            Project                            |
| MiniGPT-5    | ![Star](https://img.shields.io/github/stars/eric-ai-lab/MiniGPT-5.svg?style=social&label=Star) <br> [**MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens**](https://arxiv.org/pdf/2310.02239)                                                      |  ArXiv  | 2024-3-15  |       [GitHub](https://github.com/eric-ai-lab/MiniGPT-5)        | [Project](https://eric-ai-lab.github.io/minigpt-5.github.io/) |
| LMD          | ![Star](https://img.shields.io/github/stars/TonyLianLong/LLM-groundedDiffusion.svg?style=social&label=Star) <br> [**LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models**](https://arxiv.org/pdf/2305.13655) |  TMLR   |  2024-3-4  | [GitHub](https://github.com/TonyLianLong/LLM-groundedDiffusion) |     [Project](https://llm-grounded-diffusion.github.io/)      |
| DiffusionGPT | ![Star](https://img.shields.io/github/stars/DiffusionGPT/DiffusionGPT.svg?style=social&label=Star) <br> [**DiffusionGPT: LLM-Driven Text-to-Image Generation System**](https://arxiv.org/pdf/2401.10061)                                                                     |  ArXiv  | 2024-1-18  |     [GitHub](https://github.com/DiffusionGPT/DiffusionGPT)      |          [Project](https://diffusiongpt.github.io/)           |
| VL-GPT       | ![Star](https://img.shields.io/github/stars/AILab-CVC/VL-GPT.svg?style=social&label=Star) <br> [**VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation**](https://arxiv.org/pdf/2307.08041)                                     |  ArXiv  | 2023-12-4  |          [GitHub](https://github.com/AILab-CVC/VL-GPT)          |                            Project                            |
| CoDi-2       | ![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star) <br> [**CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**](https://arxiv.org/pdf/2311.18775)                                                                |  CVPR   | 2023-11-30 | [GitHub](https://github.com/microsoft/i-Code/tree/main/CoDi-2)  |             [Project](https://codi-2.github.io/)              |
| SEED-LLAMA   | ![Star](https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&label=Star) <br> [**Making LLaMA SEE and Draw with SEED Tokenizer**](https://arxiv.org/pdf/2310.01218)                                                                                         |  CVPR   | 2023-10-3  |           [GitHub](https://github.com/AILab-CVC/SEED)           |         [Project](https://ailab-cvc.github.io/seed/)          |
| JAM          | [**Jointly Training Large Autoregressive Multimodal Models**](https://arxiv.org/pdf/2309.15564)                                                                                                                                                                              |  ICLR   | 2023-9-28  |                             GitHub                              |                            Project                            |
| CM3Leon      | [**Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning**](https://arxiv.org/pdf/2309.02591)                                                                                                                                                        |  ArXiv  |  2023-9-5  |                             GitHub                              |                            Project                            |
| SEED         | ![Star](https://img.shields.io/github/stars/AILab-CVC/SEED.svg?style=social&label=Star) <br> [**Planting a SEED of Vision in Large Language Model**](https://arxiv.org/pdf/2307.08041)                                                                                       |  ICLR   | 2023-8-12  |           [GitHub](https://github.com/AILab-CVC/SEED)           |         [Project](https://ailab-cvc.github.io/seed/)          |
| GILL         | ![Star](https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&label=Star) <br> [**Generating Images with Multimodal Language Models**](https://arxiv.org/pdf/2305.17216)                                                                                       | NeurlPS | 2023-5-26  |           [GitHub](https://github.com/kohjingyu/gill)           |               [Project](https://jykoh.com/gill)               |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### Text-to-3D

| Name | Title                                                                                                                                          |  Venue  |   Date    |  Code  | Project |
| :--- | :--------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :-------: | :----: | :-----: |
| LI3D | [**Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback**](https://arxiv.org/pdf/2305.15808) | NeurlPS | 2023-5-26 | GitHub | Project |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### Text-to-video

| Name | Title | Venue | Date | Code | Project |
| :--- | :---- | :---: | :--: | :--: | :-----: |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

## Other applications

### Face

| Name      | Title                                                                                                                                                                                                            | Venue |    Date    |                     Code                      |                       Project                       |
| :-------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :--------: | :-------------------------------------------: | :-------------------------------------------------: |
| Face-MLLM | [**Face-MLLM: A Large Face Perception Model**](https://arxiv.org/pdf/2410.20717)                                                                                                                                 | arXiv | 2024-10-28 |                    Github                     |                       Project                       |
| ExpLLM    | [**ExpLLM: Towards Chain of Thought for Facial Expression Recognition**](https://arxiv.org/pdf/2409.02828)                                                                                                       | arXiv |  2024-9-4  |                    Github                     | [Project](https://starhiking.github.io/ExpLLM_Page) |
| EMO-LLaMA | ![Star](https://img.shields.io/github/stars/xxtars/EMO-LLaMA.svg?style=social&label=Star) <br> [**EMO-LLaMA: Enhancing Facial Emotion Understanding with Instruction Tuning**](https://arxiv.org/pdf/2408.11424) | arXiv | 2024-8-21  | [Github](https://github.com/xxtars/EMO-LLaMA) |                       Project                       |
| EmoLA     | ![Star](https://img.shields.io/github/stars/JackYFL/EmoLA.svg?style=social&label=Star) <br> [**Facial Affective Behavior Analysis with Instruction Tuning**](https://arxiv.org/pdf/2404.05052)                   | ECCV  | 2024-7-12  |  [Github](https://github.com/JackYFL/EmoLA)   |     [Project](https://johnx69.github.io/FABA/)      |
| EmoLLM    | ![Star](https://img.shields.io/github/stars/yan9qu/EmoLLM.svg?style=social&label=Star) <br> [**EmoLLM: Multimodal Emotional Understanding Meets Large Language Models**](https://arxiv.org/pdf/2406.16442)       | ArXiv | 2024-6-29  |  [Github](https://github.com/yan9qu/EmoLLM)   |                       Project                       |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

### Anomaly Detetcion

| Name  | Title                                                                                                                                                                                                                                  |  Venue  |   Date    |                      Code                      |                     Project                     |
| :---- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----: | :-------: | :--------------------------------------------: | :---------------------------------------------: |
| HAWK  | ![Star](https://img.shields.io/github/stars/jqtangust/hawk.svg?style=social&label=Star) <br> [**HAWK: Learning to Understand Open-World Video Anomalies**](https://arxiv.org/pdf/2405.16886)                                           | NeurlPS | 2024-5-27 |  [Github](https://github.com/jqtangust/hawk)   |                     Project                     |
| CUVA  | ![Star](https://img.shields.io/github/stars/fesvhtr/CUVA.svg?style=social&label=Star) <br> [**Uncovering What Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly**](https://arxiv.org/pdf/2405.00181) |  CVPR   | 2024-5-6  |   [Github](https://github.com/fesvhtr/CUVA)    |                     Project                     |
| LAVAD | ![Star](https://img.shields.io/github/stars/lucazanella/lavad.svg?style=social&label=Star) <br> [**Harnessing Large Language Models for Training-free Video Anomaly Detectiong**](https://arxiv.org/pdf/2404.05052)                    |  CVPR   | 2024-4-1  | [Github](https://github.com/lucazanella/lavad) | [Project](https://lucazanella.github.io/lavad/) |

[<u><ðŸŽ¯Back to Top></u>](#head-content)

<!-- ## <span id="head8"> Contributors </span>
Thanks to all the contributors! -->
